{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Install Transformers Library","metadata":{}},{"cell_type":"code","source":"#!pip install transformers\n#!pip install openpyxl\n# import glob\n# paths = glob.glob('../input/sentiment-classification-on-movie-reviews/*')\n# print(paths)\n!pip install bert-extractive-summarizer","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-11T11:19:21.956252Z","iopub.execute_input":"2021-08-11T11:19:21.956689Z","iopub.status.idle":"2021-08-11T11:19:30.586408Z","shell.execute_reply.started":"2021-08-11T11:19:21.956629Z","shell.execute_reply":"2021-08-11T11:19:30.585270Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting bert-extractive-summarizer\n  Downloading bert_extractive_summarizer-0.8.1-py3-none-any.whl (19 kB)\nRequirement already satisfied: spacy in /opt/conda/lib/python3.7/site-packages (from bert-extractive-summarizer) (2.3.5)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from bert-extractive-summarizer) (0.24.1)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (from bert-extractive-summarizer) (4.5.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->bert-extractive-summarizer) (2.1.0)\nRequirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->bert-extractive-summarizer) (1.19.5)\nRequirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->bert-extractive-summarizer) (1.5.4)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->bert-extractive-summarizer) (1.0.1)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy->bert-extractive-summarizer) (2.0.5)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy->bert-extractive-summarizer) (3.0.5)\nRequirement already satisfied: catalogue<1.1.0,>=0.0.7 in /opt/conda/lib/python3.7/site-packages (from spacy->bert-extractive-summarizer) (1.0.0)\nRequirement already satisfied: plac<1.2.0,>=0.9.6 in /opt/conda/lib/python3.7/site-packages (from spacy->bert-extractive-summarizer) (1.1.3)\nRequirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy->bert-extractive-summarizer) (0.7.4)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy->bert-extractive-summarizer) (1.0.5)\nRequirement already satisfied: srsly<1.1.0,>=1.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy->bert-extractive-summarizer) (1.0.5)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.7/site-packages (from spacy->bert-extractive-summarizer) (4.59.0)\nRequirement already satisfied: wasabi<1.1.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy->bert-extractive-summarizer) (0.8.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy->bert-extractive-summarizer) (49.6.0.post20210108)\nRequirement already satisfied: thinc<7.5.0,>=7.4.1 in /opt/conda/lib/python3.7/site-packages (from spacy->bert-extractive-summarizer) (7.4.5)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.7/site-packages (from spacy->bert-extractive-summarizer) (2.25.1)\nRequirement already satisfied: importlib-metadata>=0.20 in /opt/conda/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy->bert-extractive-summarizer) (3.4.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy->bert-extractive-summarizer) (3.4.1)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy->bert-extractive-summarizer) (3.7.4.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (2020.12.5)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (2.10)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (4.0.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers->bert-extractive-summarizer) (2021.3.17)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers->bert-extractive-summarizer) (3.0.12)\nRequirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers->bert-extractive-summarizer) (0.10.2)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers->bert-extractive-summarizer) (0.0.45)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers->bert-extractive-summarizer) (20.9)\nRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers->bert-extractive-summarizer) (2.4.7)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers->bert-extractive-summarizer) (1.15.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers->bert-extractive-summarizer) (7.1.2)\nInstalling collected packages: bert-extractive-summarizer\nSuccessfully installed bert-extractive-summarizer-0.8.1\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW, BertConfig, get_linear_schedule_with_warmup #BertTokenizer, BertForSequenceClassification\nimport torch\nimport pandas as pd\nimport numpy as np\nimport time\nimport datetime\nimport random\nimport plotly.express as px\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.sequence import pad_sequences\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nimport re #用以同時刪除多種不同字元\n\nfrom summarizer import Summarizer\n#from transformers import *","metadata":{"execution":{"iopub.status.busy":"2021-08-11T11:19:30.595025Z","iopub.execute_input":"2021-08-11T11:19:30.595499Z","iopub.status.idle":"2021-08-11T11:19:43.236783Z","shell.execute_reply.started":"2021-08-11T11:19:30.595448Z","shell.execute_reply":"2021-08-11T11:19:43.235703Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Load Dataset","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/sentiment-classification-on-movie-reviews/train.csv\")\nprint(df.head())\nprint(df.tail())\n# print(df.sample(10))","metadata":{"execution":{"iopub.status.busy":"2021-08-11T11:19:43.238793Z","iopub.execute_input":"2021-08-11T11:19:43.239249Z","iopub.status.idle":"2021-08-11T11:19:44.025937Z","shell.execute_reply.started":"2021-08-11T11:19:43.239204Z","shell.execute_reply":"2021-08-11T11:19:44.024972Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"      ID                                             review  sentiment\n0  41411  I watched this film because I'm a big fan of R...          0\n1  37586  It does not seem that this movie managed to pl...          1\n2   6017        Enough is not a bad movie , just mediocre .          0\n3  44656  my friend and i rented this one a few nights a...          0\n4  38711  Just about everything in this movie is wrong, ...          0\n          ID                                             review  sentiment\n29336   8019  It 's one of the most honest films ever made a...          1\n29337    453  An absorbing and unsettling psychological drama .          1\n29338  13097  Soylent Green IS...a really good movie, actual...          1\n29339  26896  There just isn't enough here. There a few funn...          0\n29340  27094  This show was absolutely terrible. For one Geo...          0\n","output_type":"stream"}]},{"cell_type":"code","source":"# Get the lists of sentences and their labels.\nsentences = df.review.values\nlabels = df.sentiment.values","metadata":{"execution":{"iopub.status.busy":"2021-08-11T11:19:44.027406Z","iopub.execute_input":"2021-08-11T11:19:44.027786Z","iopub.status.idle":"2021-08-11T11:19:44.035578Z","shell.execute_reply.started":"2021-08-11T11:19:44.027747Z","shell.execute_reply":"2021-08-11T11:19:44.034223Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# check class distribution\ndf.sentiment.value_counts(normalize = True)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T11:19:44.037654Z","iopub.execute_input":"2021-08-11T11:19:44.038545Z","iopub.status.idle":"2021-08-11T11:19:44.068782Z","shell.execute_reply.started":"2021-08-11T11:19:44.038491Z","shell.execute_reply":"2021-08-11T11:19:44.067751Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"1    0.509662\n0    0.490338\nName: sentiment, dtype: float64"},"metadata":{}}]},{"cell_type":"code","source":"#print(train_inputs.device)\nif torch.cuda.is_available():\n    print (\"yes\")\nelse:\n    print(\"no\")\n    \ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2021-08-11T11:19:44.070461Z","iopub.execute_input":"2021-08-11T11:19:44.070941Z","iopub.status.idle":"2021-08-11T11:19:44.123562Z","shell.execute_reply.started":"2021-08-11T11:19:44.070894Z","shell.execute_reply":"2021-08-11T11:19:44.122395Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"yes\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 將sentence轉成token","metadata":{}},{"cell_type":"markdown","source":"1. 句子前處理","metadata":{}},{"cell_type":"code","source":"for i in range(0,29341):\n    sentences[i] = re.sub('<br />', ' ', sentences[i]) #去除特定字元符號\n    sentences[i] = re.sub('--(-+)', '--', sentences[i])\n    #sentences[i] = re.sub('\\((2([0-9][0-9][0-9])|1([0-9][0-9][0-9]))\\)', ' ', sentences[i])\n    sentences[i] = re.sub('\\?\\?(\\?+)', '??', sentences[i])\n    sentences[i] = re.sub('!!(!+)', '!!', sentences[i])\n    \n    sentences[i] = re.sub('I\\'m', 'I am', sentences[i])\n    sentences[i] = re.sub('can\\'t', 'can not', sentences[i])\n    sentences[i] = re.sub('n\\'t', ' not', sentences[i])\n    \n    if( i % 10000 == 0 ):\n        print(i)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T11:19:44.125538Z","iopub.execute_input":"2021-08-11T11:19:44.126268Z","iopub.status.idle":"2021-08-11T11:19:44.665382Z","shell.execute_reply.started":"2021-08-11T11:19:44.126220Z","shell.execute_reply":"2021-08-11T11:19:44.664302Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"0\n10000\n20000\n","output_type":"stream"}]},{"cell_type":"code","source":"# get length of all the messages\nseq_len = [len(i.split()) for i in sentences]\n\npd.Series(seq_len).hist(bins = 30)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T11:19:44.668736Z","iopub.execute_input":"2021-08-11T11:19:44.669155Z","iopub.status.idle":"2021-08-11T11:19:45.318187Z","shell.execute_reply.started":"2021-08-11T11:19:44.669120Z","shell.execute_reply":"2021-08-11T11:19:45.317118Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"<AxesSubplot:>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"counter_t = 0\nfor i in range(0,29341):\n    if(len(sentences[i].split())>1100): # 句子長度超過1100字的  \n        counter_t += 1\n        print(i, \":\", len(sentences[i].split()))\n        #print(i)\nprint(\"Total:\", counter_t)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T11:19:45.320241Z","iopub.execute_input":"2021-08-11T11:19:45.320566Z","iopub.status.idle":"2021-08-11T11:19:45.730041Z","shell.execute_reply.started":"2021-08-11T11:19:45.320531Z","shell.execute_reply":"2021-08-11T11:19:45.729062Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"993 : 1319\n1927 : 1727\n2207 : 1833\n3989 : 1145\n8162 : 1171\n15635 : 1106\n20712 : 2098\n21227 : 1158\n23964 : 1144\n29108 : 1149\nTotal: 10\n","output_type":"stream"}]},{"cell_type":"code","source":"# from summarizer import Summarizer\n\n# model = Summarizer(\n# #     custom_model=custom_model,\n# #     custom_tokenizer=custom_tokenizer,\n# #     sentence_handler = SentenceHandler(language=Chinese),\n# )","metadata":{"execution":{"iopub.status.busy":"2021-08-11T11:19:45.731608Z","iopub.execute_input":"2021-08-11T11:19:45.732049Z","iopub.status.idle":"2021-08-11T11:19:45.739427Z","shell.execute_reply.started":"2021-08-11T11:19:45.732002Z","shell.execute_reply":"2021-08-11T11:19:45.738182Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# result = model(\n#     body = sentences[20712], # The string body that you want to summarize (str)\n#     #ratio = 0.2, # The ratio of sentences that you want for the final summary (float)\n#     min_length = 40, # Parameter to specify to remove sentences that are less than 40 characters (int)\n#     #max_length = , # Parameter to specify to remove sentences greater than the max length (int)\n#     num_sentences = 8, # Number of sentences to use. Overrides ratio if supplied.\n# )\n# print(result)\n# print(len(result.split()))","metadata":{"execution":{"iopub.status.busy":"2021-08-11T11:19:45.743527Z","iopub.execute_input":"2021-08-11T11:19:45.743891Z","iopub.status.idle":"2021-08-11T11:19:45.750688Z","shell.execute_reply.started":"2021-08-11T11:19:45.743860Z","shell.execute_reply":"2021-08-11T11:19:45.749619Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# for i in range(0,29341):\n#     if(len(sentences[i].split())>250): # 句子長度超過250字的  \n#         sentences_temp = model( sentences[i] )\n#         sentences[i] = sentences_temp\n#     if(i%200 == 0):\n#         print(i)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T11:19:45.752705Z","iopub.execute_input":"2021-08-11T11:19:45.753615Z","iopub.status.idle":"2021-08-11T11:19:45.758667Z","shell.execute_reply.started":"2021-08-11T11:19:45.753564Z","shell.execute_reply":"2021-08-11T11:19:45.757473Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# for i in range(0,29341): # 句子長度超過250字的，取最前段的250字    \n#     if(len(sentences[i].split())>250):\n#         sentences_temp = sentences[i].split()\n#         sentences[i] = \" \".join(sentences_temp[0:250])","metadata":{"execution":{"iopub.status.busy":"2021-08-11T11:19:45.760545Z","iopub.execute_input":"2021-08-11T11:19:45.761412Z","iopub.status.idle":"2021-08-11T11:19:45.768154Z","shell.execute_reply.started":"2021-08-11T11:19:45.761359Z","shell.execute_reply":"2021-08-11T11:19:45.766942Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# counter = 0\n# for i in range(0,29341):\n#     if(len(sentences[i].split())>=280):\n#         counter += 1\n#         #print(len(sentences[i].split()))\n# print(counter)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T11:19:45.770088Z","iopub.execute_input":"2021-08-11T11:19:45.770706Z","iopub.status.idle":"2021-08-11T11:19:45.778021Z","shell.execute_reply.started":"2021-08-11T11:19:45.770656Z","shell.execute_reply":"2021-08-11T11:19:45.776704Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"2. Tokenize Dataset","metadata":{}},{"cell_type":"code","source":"## 載入Roberta tokenizer\nfrom transformers import RobertaTokenizerFast\nprint('==== Loading RobertaTokenizerFast ====\\n')\ntokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")","metadata":{"execution":{"iopub.status.busy":"2021-08-11T11:19:45.779738Z","iopub.execute_input":"2021-08-11T11:19:45.780233Z","iopub.status.idle":"2021-08-11T11:19:46.921669Z","shell.execute_reply.started":"2021-08-11T11:19:45.780191Z","shell.execute_reply":"2021-08-11T11:19:46.920577Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"==== Loading RobertaTokenizerFast ====\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d8aa53160af44cea1d3212c20988996"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9874c2029ef64ae48dc71281da228062"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13e9035c3a2844fb942f1695adf4c79a"}},"metadata":{}}]},{"cell_type":"code","source":"input_ids = []\nidx_counter = 0\nfor sent in sentences:\n    encoded_sent = tokenizer.encode(\n        sent,\n        add_special_tokens = True, # Add '[CLS]' and '[SEP]' \n    )\n    input_ids.append(encoded_sent)\n    if(idx_counter%5000 == 0):\n        print(idx_counter)\n    idx_counter += 1\n    \nprint('==== Tokenized Sentences ====\\n')  \nprint('Original: ', sentences[0])\nprint('Token IDs:', input_ids[0])","metadata":{"execution":{"iopub.status.busy":"2021-08-11T11:19:46.923204Z","iopub.execute_input":"2021-08-11T11:19:46.923626Z","iopub.status.idle":"2021-08-11T11:20:18.541931Z","shell.execute_reply.started":"2021-08-11T11:19:46.923584Z","shell.execute_reply":"2021-08-11T11:20:18.540874Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (549 > 512). Running this sequence through the model will result in indexing errors\n","output_type":"stream"},{"name":"stdout","text":"0\n5000\n10000\n15000\n20000\n25000\n==== Tokenized Sentences ====\n\nOriginal:  I watched this film because I am a big fan of River Phoenix and Joaquin Phoenix. I thought I would give their sister a try, Rain Phoenix. I regret checking it out. She was embarrasing and the film just has this weird plot if thats what you want to call it. Sissy was just weird and Jellybean just sits on a toilet who both sleep with this old man in the mountains, whats going on? I have never been so unsatisfied in my life. It was just total rubbish. I can not believe that the actors agreed to do such a waste of film, money, time and space. Have Sissy being 'beautiful' didnt get to me. I thought she was everything but that. Those thumbs were just stupid, and why do we care if she can hitchhike? WHATS THE POINT?? 0 out of 10, shame the poll doesnt have a 0, doesnt even deserve a 1. Hopefully, Rain is better in other films, I forgive her for this one performance, I mean I wouldnt do much better with that film.\nToken IDs: [0, 100, 3996, 42, 822, 142, 38, 524, 10, 380, 2378, 9, 1995, 5524, 8, 3889, 23186, 5524, 4, 38, 802, 38, 74, 492, 49, 2761, 10, 860, 6, 8699, 5524, 4, 38, 9917, 8405, 24, 66, 4, 264, 21, 18484, 6166, 7913, 8, 5, 822, 95, 34, 42, 7735, 6197, 114, 45365, 99, 47, 236, 7, 486, 24, 4, 208, 20157, 21, 95, 7735, 8, 39912, 16595, 95, 6476, 15, 10, 11471, 54, 258, 3581, 19, 42, 793, 313, 11, 5, 9787, 6, 45676, 164, 15, 116, 38, 33, 393, 57, 98, 36010, 2550, 11, 127, 301, 4, 85, 21, 95, 746, 22051, 4, 38, 64, 45, 679, 14, 5, 5552, 1507, 7, 109, 215, 10, 3844, 9, 822, 6, 418, 6, 86, 8, 980, 4, 6319, 208, 20157, 145, 128, 28878, 16170, 108, 46405, 120, 7, 162, 4, 38, 802, 79, 21, 960, 53, 14, 4, 2246, 26081, 58, 95, 12103, 6, 8, 596, 109, 52, 575, 114, 79, 64, 30601, 298, 4348, 116, 13331, 104, 1941, 17182, 17831, 28749, 321, 66, 9, 158, 6, 9208, 5, 2902, 47320, 33, 10, 321, 6, 47320, 190, 6565, 10, 112, 4, 13088, 6, 8699, 16, 357, 11, 97, 3541, 6, 38, 20184, 69, 13, 42, 65, 819, 6, 38, 1266, 38, 74, 3999, 109, 203, 357, 19, 14, 822, 4, 2]\n","output_type":"stream"}]},{"cell_type":"code","source":"# get length of all the Tokenized Sentences\ntokenized_seq_len = [len(i) for i in input_ids]\n\npd.Series(tokenized_seq_len).hist(bins = 30)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T11:20:18.543569Z","iopub.execute_input":"2021-08-11T11:20:18.544021Z","iopub.status.idle":"2021-08-11T11:20:18.819418Z","shell.execute_reply.started":"2021-08-11T11:20:18.543974Z","shell.execute_reply":"2021-08-11T11:20:18.818416Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"<AxesSubplot:>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"# for i in input_ids:\n#     if(len(i)>400):\n#         print(len(i))","metadata":{"execution":{"iopub.status.busy":"2021-08-11T11:20:18.820936Z","iopub.execute_input":"2021-08-11T11:20:18.821388Z","iopub.status.idle":"2021-08-11T11:20:18.825830Z","shell.execute_reply.started":"2021-08-11T11:20:18.821344Z","shell.execute_reply":"2021-08-11T11:20:18.824671Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Tokenized Sentences長度超過350字的，取最前段的350字\nfor i in range(0,29341):\n    if(len(input_ids[i])>350):\n        temp = input_ids[i][0:349]\n        temp.append(2) # BERT:102 RoBERTa:2\n        input_ids[i] = temp","metadata":{"execution":{"iopub.status.busy":"2021-08-11T11:20:18.827593Z","iopub.execute_input":"2021-08-11T11:20:18.828334Z","iopub.status.idle":"2021-08-11T11:20:18.881464Z","shell.execute_reply.started":"2021-08-11T11:20:18.828274Z","shell.execute_reply":"2021-08-11T11:20:18.880316Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# get length of all the processed encode Sentences\nseq_len_encode = [len(i) for i in input_ids]\n\npd.Series(seq_len_encode).hist(bins = 30)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T11:20:18.883195Z","iopub.execute_input":"2021-08-11T11:20:18.883884Z","iopub.status.idle":"2021-08-11T11:20:19.185488Z","shell.execute_reply.started":"2021-08-11T11:20:18.883835Z","shell.execute_reply":"2021-08-11T11:20:19.184244Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"<AxesSubplot:>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"max_seq_len = max([len(sen) for sen in input_ids])\nprint('Max sentence length: ', max_seq_len)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T11:20:19.187101Z","iopub.execute_input":"2021-08-11T11:20:19.187579Z","iopub.status.idle":"2021-08-11T11:20:19.197771Z","shell.execute_reply.started":"2021-08-11T11:20:19.187538Z","shell.execute_reply":"2021-08-11T11:20:19.196402Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Max sentence length:  350\n","output_type":"stream"}]},{"cell_type":"code","source":"# for i in range(0,29341):\n#     if(len(input_ids[i])== 506):\n#     #if(len(input_ids[i])>508 and len(input_ids[i])<510):\n#         print(i,': encoded len:',len(input_ids[i]),'sentence len:',len(sentences[i].split()))\n#         print(sentences[i])","metadata":{"execution":{"iopub.status.busy":"2021-08-11T11:20:19.199538Z","iopub.execute_input":"2021-08-11T11:20:19.200341Z","iopub.status.idle":"2021-08-11T11:20:19.208015Z","shell.execute_reply.started":"2021-08-11T11:20:19.200293Z","shell.execute_reply":"2021-08-11T11:20:19.206606Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"3. Padding","metadata":{}},{"cell_type":"code","source":"MAX_LEN = max_seq_len\nprint('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\nprint('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n# Pad our input tokens with value 0.\n# \"post\" indicates that we want to pad and truncate at the end of the sequence,\n# as opposed to the beginning.\ninput_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n                          value=0, truncating=\"post\", padding=\"post\")\nprint('Done.')","metadata":{"execution":{"iopub.status.busy":"2021-08-11T11:20:19.210253Z","iopub.execute_input":"2021-08-11T11:20:19.210825Z","iopub.status.idle":"2021-08-11T11:20:20.807986Z","shell.execute_reply.started":"2021-08-11T11:20:19.210779Z","shell.execute_reply":"2021-08-11T11:20:20.807018Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"\nPadding/truncating all sentences to 350 values...\n\nPadding token: \"<pad>\", ID: 1\nDone.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"4. Attention Mask","metadata":{}},{"cell_type":"code","source":"# attention masks\nattention_masks = []\n\nfor sent in input_ids:\n    att_mask = [int(token_id > 0) for token_id in sent]\n    attention_masks.append(att_mask)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T11:20:20.811306Z","iopub.execute_input":"2021-08-11T11:20:20.811709Z","iopub.status.idle":"2021-08-11T11:20:31.865094Z","shell.execute_reply.started":"2021-08-11T11:20:20.811674Z","shell.execute_reply":"2021-08-11T11:20:31.864098Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"# Split train dataset into train, validation and test sets","metadata":{}},{"cell_type":"code","source":"## 準備訓練集、驗證集 \ntrain_inputs, validation_inputs, train_labels, validation_labels = train_test_split(\n                input_ids, labels, random_state = 1680, \n                test_size=0.2,\n                stratify=labels,\n                )\n\n# 同樣masks\ntrain_masks, validation_masks, _, _ = train_test_split(\n                attention_masks, labels, random_state = 1680,\n                test_size=0.2,\n                stratify=labels,\n                )","metadata":{"execution":{"iopub.status.busy":"2021-08-11T11:20:31.866705Z","iopub.execute_input":"2021-08-11T11:20:31.867145Z","iopub.status.idle":"2021-08-11T11:20:31.962088Z","shell.execute_reply.started":"2021-08-11T11:20:31.867082Z","shell.execute_reply":"2021-08-11T11:20:31.961075Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# 轉換成torch tensors\ntrain_inputs = torch.tensor(train_inputs)\nvalidation_inputs = torch.tensor(validation_inputs)\ntrain_labels = torch.tensor(train_labels)\nvalidation_labels = torch.tensor(validation_labels)\ntrain_masks = torch.tensor(train_masks)\nvalidation_masks = torch.tensor(validation_masks)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T11:20:31.967522Z","iopub.execute_input":"2021-08-11T11:20:31.967915Z","iopub.status.idle":"2021-08-11T11:20:33.375972Z","shell.execute_reply.started":"2021-08-11T11:20:31.967864Z","shell.execute_reply":"2021-08-11T11:20:33.374906Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"train_inputs = train_inputs.to(device)\nvalidation_inputs = validation_inputs.to(device)\ntrain_labels = train_labels.to(device)\nvalidation_labels = validation_labels.to(device)\ntrain_masks = train_masks.to(device)\nvalidation_masks = validation_masks.to(device)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T11:20:33.382039Z","iopub.execute_input":"2021-08-11T11:20:33.384603Z","iopub.status.idle":"2021-08-11T11:20:39.562158Z","shell.execute_reply.started":"2021-08-11T11:20:33.384547Z","shell.execute_reply":"2021-08-11T11:20:39.561204Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# The DataLoader\nbatch_size = 8\n\n# 建立訓練集的DataLoader\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\n# 建立驗證集的DataLoader\nvalidation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\nvalidation_sampler = SequentialSampler(validation_data)\nvalidation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T11:20:39.563669Z","iopub.execute_input":"2021-08-11T11:20:39.564072Z","iopub.status.idle":"2021-08-11T11:20:39.571034Z","shell.execute_reply.started":"2021-08-11T11:20:39.564026Z","shell.execute_reply":"2021-08-11T11:20:39.569890Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"# Import RoBERTa-base pretrained model","metadata":{}},{"cell_type":"code","source":"## Load RobertaForSequenceClassification, the pretrained Roberta model\nmodel = RobertaForSequenceClassification.from_pretrained(\n        'siebert/sentiment-roberta-large-english', #'cardiffnlp/twitter-roberta-base-sentiment', # 'roberta-base'\n        num_labels = 2,\n        output_attentions = False,\n        output_hidden_states = False,\n        )","metadata":{"execution":{"iopub.status.busy":"2021-08-11T11:20:39.572746Z","iopub.execute_input":"2021-08-11T11:20:39.573374Z","iopub.status.idle":"2021-08-11T11:21:33.411246Z","shell.execute_reply.started":"2021-08-11T11:20:39.573331Z","shell.execute_reply":"2021-08-11T11:21:33.410214Z"},"trusted":true},"execution_count":29,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/687 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80efba574e3d4911a0764fda34ac0ae9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.42G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60ed15a566e041c6aea6c615563b2f74"}},"metadata":{}}]},{"cell_type":"code","source":"# Get all of the model's parameters as a list of tuples.\nparams = list(model.named_parameters())\nprint('The RoBERTa model has {:} different named parameters.\\n'.format(len(params)))\nprint('==== Embedding Layer ====\\n')\nfor p in params[0:5]:\n    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\nprint('\\n==== First Transformer ====\\n')\nfor p in params[5:21]:\n    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\nprint('\\n==== Output Layer ====\\n')\nfor p in params[-4:]:\n    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n\n# to cuda\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T11:21:33.412724Z","iopub.execute_input":"2021-08-11T11:21:33.413327Z","iopub.status.idle":"2021-08-11T11:21:33.839709Z","shell.execute_reply.started":"2021-08-11T11:21:33.413286Z","shell.execute_reply":"2021-08-11T11:21:33.838812Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"The RoBERTa model has 393 different named parameters.\n\n==== Embedding Layer ====\n\nroberta.embeddings.word_embeddings.weight               (50265, 1024)\nroberta.embeddings.position_embeddings.weight            (514, 1024)\nroberta.embeddings.token_type_embeddings.weight            (1, 1024)\nroberta.embeddings.LayerNorm.weight                          (1024,)\nroberta.embeddings.LayerNorm.bias                            (1024,)\n\n==== First Transformer ====\n\nroberta.encoder.layer.0.attention.self.query.weight     (1024, 1024)\nroberta.encoder.layer.0.attention.self.query.bias            (1024,)\nroberta.encoder.layer.0.attention.self.key.weight       (1024, 1024)\nroberta.encoder.layer.0.attention.self.key.bias              (1024,)\nroberta.encoder.layer.0.attention.self.value.weight     (1024, 1024)\nroberta.encoder.layer.0.attention.self.value.bias            (1024,)\nroberta.encoder.layer.0.attention.output.dense.weight   (1024, 1024)\nroberta.encoder.layer.0.attention.output.dense.bias          (1024,)\nroberta.encoder.layer.0.attention.output.LayerNorm.weight      (1024,)\nroberta.encoder.layer.0.attention.output.LayerNorm.bias      (1024,)\nroberta.encoder.layer.0.intermediate.dense.weight       (4096, 1024)\nroberta.encoder.layer.0.intermediate.dense.bias              (4096,)\nroberta.encoder.layer.0.output.dense.weight             (1024, 4096)\nroberta.encoder.layer.0.output.dense.bias                    (1024,)\nroberta.encoder.layer.0.output.LayerNorm.weight              (1024,)\nroberta.encoder.layer.0.output.LayerNorm.bias                (1024,)\n\n==== Output Layer ====\n\nclassifier.dense.weight                                 (1024, 1024)\nclassifier.dense.bias                                        (1024,)\nclassifier.out_proj.weight                                 (2, 1024)\nclassifier.out_proj.bias                                        (2,)\n","output_type":"stream"},{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"RobertaForSequenceClassification(\n  (roberta): RobertaModel(\n    (embeddings): RobertaEmbeddings(\n      (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 1024)\n      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): RobertaEncoder(\n      (layer): ModuleList(\n        (0): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (12): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (13): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (14): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (15): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (16): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (17): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (18): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (19): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (20): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (21): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (22): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (23): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (classifier): RobertaClassificationHead(\n    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (out_proj): Linear(in_features=1024, out_features=2, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"optimizer = AdamW(model.parameters(),\n                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n                  eps = 1e-8, # args.adam_epsilon  - default is 1e-8.\n                  weight_decay = 0.01,# default is 0.\n                )\n                \nepochs = 3 # authors recommend between 2 and 4\ntotal_steps = len(train_dataloader) * epochs\n# Create the learning rate scheduler.\nscheduler = get_linear_schedule_with_warmup(\n                optimizer, \n                num_warmup_steps = 0, # Default value\n                num_training_steps = total_steps\n                )","metadata":{"execution":{"iopub.status.busy":"2021-08-11T11:21:33.841250Z","iopub.execute_input":"2021-08-11T11:21:33.841646Z","iopub.status.idle":"2021-08-11T11:21:33.854488Z","shell.execute_reply.started":"2021-08-11T11:21:33.841606Z","shell.execute_reply":"2021-08-11T11:21:33.853017Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"def flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) / len(labels_flat)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T11:21:33.856319Z","iopub.execute_input":"2021-08-11T11:21:33.856944Z","iopub.status.idle":"2021-08-11T11:21:33.866047Z","shell.execute_reply.started":"2021-08-11T11:21:33.856902Z","shell.execute_reply":"2021-08-11T11:21:33.865075Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"def format_time(elapsed):\n    '''\n    Takes a time in seconds and returns a string hh:mm:ss\n    '''\n    # Round to the nearest second.\n    elapsed_rounded = int(round((elapsed)))\n    \n    # Format as hh:mm:ss\n    return str(datetime.timedelta(seconds=elapsed_rounded))","metadata":{"execution":{"iopub.status.busy":"2021-08-11T11:21:33.867606Z","iopub.execute_input":"2021-08-11T11:21:33.868069Z","iopub.status.idle":"2021-08-11T11:21:33.875835Z","shell.execute_reply.started":"2021-08-11T11:21:33.868026Z","shell.execute_reply":"2021-08-11T11:21:33.874799Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"seed_val = 64\n\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T11:21:33.877533Z","iopub.execute_input":"2021-08-11T11:21:33.877969Z","iopub.status.idle":"2021-08-11T11:21:33.886314Z","shell.execute_reply.started":"2021-08-11T11:21:33.877924Z","shell.execute_reply":"2021-08-11T11:21:33.885161Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"loss_values = []\naccuracy_values = []","metadata":{"execution":{"iopub.status.busy":"2021-08-11T11:21:33.887965Z","iopub.execute_input":"2021-08-11T11:21:33.888573Z","iopub.status.idle":"2021-08-11T11:21:33.896902Z","shell.execute_reply.started":"2021-08-11T11:21:33.888528Z","shell.execute_reply":"2021-08-11T11:21:33.895789Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"# Train Process","metadata":{}},{"cell_type":"code","source":"# 儲存訓練和評估的 loss、準確率、訓練時長等統計指標, \ntraining_stats = []\n\n# 統計整個訓練時長\ntotal_t0 = time.time()\n\n# For each Training epoch\nfor epoch_i in range(0, epochs):\n    \n    # ========================================\n    #               Training\n    # ========================================\n    print(\"\")\n    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n    print('======== Training ========\\n')    \n    t0 = time.time()# Measure how long the training epoch takes.    \n    total_loss = 0 # Reset the total loss for this epoch.\n    \n    # 將模型設定為訓練模式。這裡並不是呼叫訓練介面的意思\n    # dropout、batchnorm 層在訓練和測試模式下的表現是不同的\n    # 啟用 BatchNormalization 和 Dropout\n    model.train()\n    \n    # 訓練集小批量迭代\n    for step, batch in enumerate(train_dataloader):\n        \n        # 每經過N次迭代，就輸出進度資訊\n        if step % 200 == 0 and not step == 0:\n            elapsed = format_time(time.time() - t0)\n            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n        \n#         b_input_ids = batch[0]#.to(device)\n#         b_input_mask = batch[1]#.to(device)\n#         b_labels = batch[2]#.to(device)\n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n        \n        # 每次計算梯度前，都需要將梯度清 0，因為 pytorch 的梯度是累加的\n        model.zero_grad()\n        \n        # 前向傳播\n        # 該函式會根據不同的引數，會返回不同的值。本例中,會返回loss和logits--模型的預測結果\n        outputs = model(b_input_ids, \n                    token_type_ids=None, \n                    attention_mask=b_input_mask, \n                    labels=b_labels)\n        \n        loss = outputs[0]\n        # 累加 loss\n        total_loss += loss.item()\n        #_, predicted = torch.max(outputs.logits, 1)\n        #total_accuracy += (predicted == b_labels).sum().item()\n        \n        # 反向傳播\n        loss.backward()\n         # 梯度裁剪，避免出現梯度爆炸情況\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        # 更新引數\n        optimizer.step()\n        # 更新學習率\n        scheduler.step()\n    # 平均訓練誤差   \n    avg_train_loss = total_loss / len(train_dataloader)\n    #avg_train_accuracy = total_accuracy / len(train_dataloader)\n    \n    loss_values.append(avg_train_loss)\n    #accuracy_values.append(avg_train_accuracy)\n    print(\"\")\n    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n    #print(\"  Average training accuracy: {0:.2f}\".format(avg_train_accuracy))\n    training_time = format_time(time.time() - t0)\n    print(\"  Training epcoh took: {:}\".format(training_time))\n    \n    # ========================================\n    #               Validation\n    # ========================================\n    \n    print(\"\")\n    print(\"======== Running Validation ========\")\n    t0 = time.time()\n    \n    # 設定模型為評估模式\n    # 不啟用 BatchNormalization 和 Dropout\n    model.eval()\n    \n    eval_loss, eval_accuracy = 0, 0\n    nb_eval_steps, nb_eval_examples = 0, 0\n    \n    for batch in validation_dataloader:\n#         batch = tuple(t for t in batch)#.to(device)\n        # 將輸入資料載入到 gpu 中\n        batch = tuple(t.to(device) for t in batch)\n        b_input_ids, b_input_mask, b_labels = batch\n        \n        # 評估的時候不需要更新引數、計算梯度\n        with torch.no_grad():\n            outputs = model(b_input_ids, \n                        token_type_ids = None, \n                        attention_mask = b_input_mask)\n        logits = outputs[0]\n        # 累加 loss\n        eval_loss += loss.item()\n        # 將預測結果和 labels 載入到 cpu 中計算\n        logits = logits.detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n        # 計算準確率\n        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n        \n        eval_accuracy += tmp_eval_accuracy\n        nb_eval_steps += 1\n    \n    avg_val_loss = eval_loss / nb_eval_steps # len(validation_dataloader)\n    avg_val_accuracy = eval_accuracy/nb_eval_steps\n    validation_time = format_time(time.time() - t0)\n    print(\"\")\n    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n    print(\"  Loss: {0:.2f}\".format(avg_val_loss))\n    print(\"  Validation took: {:}\".format(validation_time))\n    \n    # 記錄本次 epoch 的所有統計資訊\n    training_stats.append(\n        {\n            'epoch': epoch_i + 1,\n            'Training Loss': avg_train_loss,\n            'Valid. Loss': avg_val_loss,\n            'Valid. Accur.': avg_val_accuracy,\n            'Training Time': training_time,\n            'Validation Time': validation_time\n        }\n    )\n    \nprint(\"\")\nprint(\"Training complete!\")\nprint(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))","metadata":{"execution":{"iopub.status.busy":"2021-08-11T11:21:33.898916Z","iopub.execute_input":"2021-08-11T11:21:33.899731Z","iopub.status.idle":"2021-08-11T14:02:05.444688Z","shell.execute_reply.started":"2021-08-11T11:21:33.899682Z","shell.execute_reply":"2021-08-11T14:02:05.443493Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"\n======== Epoch 1 / 3 ========\n======== Training ========\n\n  Batch   200  of  2,934.    Elapsed: 0:03:24.\n  Batch   400  of  2,934.    Elapsed: 0:06:48.\n  Batch   600  of  2,934.    Elapsed: 0:10:11.\n  Batch   800  of  2,934.    Elapsed: 0:13:34.\n  Batch 1,000  of  2,934.    Elapsed: 0:16:57.\n  Batch 1,200  of  2,934.    Elapsed: 0:20:21.\n  Batch 1,400  of  2,934.    Elapsed: 0:23:44.\n  Batch 1,600  of  2,934.    Elapsed: 0:27:08.\n  Batch 1,800  of  2,934.    Elapsed: 0:30:31.\n  Batch 2,000  of  2,934.    Elapsed: 0:33:55.\n  Batch 2,200  of  2,934.    Elapsed: 0:37:18.\n  Batch 2,400  of  2,934.    Elapsed: 0:40:42.\n  Batch 2,600  of  2,934.    Elapsed: 0:44:05.\n  Batch 2,800  of  2,934.    Elapsed: 0:47:28.\n\n  Average training loss: 0.31\n  Training epcoh took: 0:49:44\n\n======== Running Validation ========\n\n  Accuracy: 0.95\n  Loss: 0.00\n  Validation took: 0:03:56\n\n======== Epoch 2 / 3 ========\n======== Training ========\n\n  Batch   200  of  2,934.    Elapsed: 0:03:23.\n  Batch   400  of  2,934.    Elapsed: 0:06:46.\n  Batch   600  of  2,934.    Elapsed: 0:10:09.\n  Batch   800  of  2,934.    Elapsed: 0:13:32.\n  Batch 1,000  of  2,934.    Elapsed: 0:16:55.\n  Batch 1,200  of  2,934.    Elapsed: 0:20:17.\n  Batch 1,400  of  2,934.    Elapsed: 0:23:40.\n  Batch 1,600  of  2,934.    Elapsed: 0:27:03.\n  Batch 1,800  of  2,934.    Elapsed: 0:30:26.\n  Batch 2,000  of  2,934.    Elapsed: 0:33:48.\n  Batch 2,200  of  2,934.    Elapsed: 0:37:11.\n  Batch 2,400  of  2,934.    Elapsed: 0:40:33.\n  Batch 2,600  of  2,934.    Elapsed: 0:43:56.\n  Batch 2,800  of  2,934.    Elapsed: 0:47:18.\n\n  Average training loss: 0.20\n  Training epcoh took: 0:49:33\n\n======== Running Validation ========\n\n  Accuracy: 0.95\n  Loss: 1.80\n  Validation took: 0:03:56\n\n======== Epoch 3 / 3 ========\n======== Training ========\n\n  Batch   200  of  2,934.    Elapsed: 0:03:22.\n  Batch   400  of  2,934.    Elapsed: 0:06:44.\n  Batch   600  of  2,934.    Elapsed: 0:10:06.\n  Batch   800  of  2,934.    Elapsed: 0:13:28.\n  Batch 1,000  of  2,934.    Elapsed: 0:16:50.\n  Batch 1,200  of  2,934.    Elapsed: 0:20:13.\n  Batch 1,400  of  2,934.    Elapsed: 0:23:35.\n  Batch 1,600  of  2,934.    Elapsed: 0:26:57.\n  Batch 1,800  of  2,934.    Elapsed: 0:30:20.\n  Batch 2,000  of  2,934.    Elapsed: 0:33:42.\n  Batch 2,200  of  2,934.    Elapsed: 0:37:04.\n  Batch 2,400  of  2,934.    Elapsed: 0:40:26.\n  Batch 2,600  of  2,934.    Elapsed: 0:43:48.\n  Batch 2,800  of  2,934.    Elapsed: 0:47:10.\n\n  Average training loss: 0.10\n  Training epcoh took: 0:49:26\n\n======== Running Validation ========\n\n  Accuracy: 0.96\n  Loss: 0.88\n  Validation took: 0:03:56\n\nTraining complete!\nTotal training took 2:40:32 (h:mm:ss)\n","output_type":"stream"}]},{"cell_type":"code","source":"f = pd.DataFrame(loss_values)\nf.columns=['Loss']\nfig = px.line(f, x=f.index, y=f.Loss)\nfig.update_layout(title='Training loss of the Model',\n                   xaxis_title='Epoch',\n                   yaxis_title='Loss')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-11T14:02:05.446377Z","iopub.execute_input":"2021-08-11T14:02:05.446984Z","iopub.status.idle":"2021-08-11T14:02:06.412351Z","shell.execute_reply.started":"2021-08-11T14:02:05.446934Z","shell.execute_reply":"2021-08-11T14:02:06.410165Z"},"trusted":true},"execution_count":37,"outputs":[{"output_type":"display_data","data":{"text/html":"        <script type=\"text/javascript\">\n        window.PlotlyConfig = {MathJaxConfig: 'local'};\n        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n        if (typeof require !== 'undefined') {\n        require.undef(\"plotly\");\n        requirejs.config({\n            paths: {\n                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n            }\n        });\n        require(['plotly'], function(Plotly) {\n            window._Plotly = Plotly;\n        });\n        }\n        </script>\n        "},"metadata":{}},{"output_type":"display_data","data":{"text/html":"<div>                            <div id=\"6cf9b41d-f412-4ea8-b6b1-e80224cdedff\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"6cf9b41d-f412-4ea8-b6b1-e80224cdedff\")) {                    Plotly.newPlot(                        \"6cf9b41d-f412-4ea8-b6b1-e80224cdedff\",                        [{\"hovertemplate\": \"index=%{x}<br>Loss=%{y}<extra></extra>\", \"legendgroup\": \"\", \"line\": {\"color\": \"#636efa\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"\", \"orientation\": \"v\", \"showlegend\": false, \"type\": \"scatter\", \"x\": [0, 1, 2], \"xaxis\": \"x\", \"y\": [0.3106960635581489, 0.2044020569571021, 0.10278070472201992], \"yaxis\": \"y\"}],                        {\"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"autotypenumbers\": \"strict\", \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Training loss of the Model\"}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Epoch\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Loss\"}}},                        {\"responsive\": true}                    ).then(function(){\n                            \nvar gd = document.getElementById('6cf9b41d-f412-4ea8-b6b1-e80224cdedff');\nvar x = new MutationObserver(function (mutations, observer) {{\n        var display = window.getComputedStyle(gd).display;\n        if (!display || display === 'none') {{\n            console.log([gd, 'removed!']);\n            Plotly.purge(gd);\n            observer.disconnect();\n        }}\n}});\n\n// Listen for the removal of the full notebook cells\nvar notebookContainer = gd.closest('#notebook-container');\nif (notebookContainer) {{\n    x.observe(notebookContainer, {childList: true});\n}}\n\n// Listen for the clearing of the current output cell\nvar outputEl = gd.closest('.output');\nif (outputEl) {{\n    x.observe(outputEl, {childList: true});\n}}\n\n                        })                };                });            </script>        </div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Save weight","metadata":{}},{"cell_type":"code","source":"torch.save(model, './model.pth')","metadata":{"execution":{"iopub.status.busy":"2021-08-11T14:02:06.413975Z","iopub.execute_input":"2021-08-11T14:02:06.414436Z","iopub.status.idle":"2021-08-11T14:02:09.883649Z","shell.execute_reply.started":"2021-08-11T14:02:06.414386Z","shell.execute_reply":"2021-08-11T14:02:09.882440Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"# 保存訓練好的權重\ntorch.save(model.state_dict(), './model_state_dict.pt')","metadata":{"execution":{"iopub.status.busy":"2021-08-11T14:02:09.889740Z","iopub.execute_input":"2021-08-11T14:02:09.892992Z","iopub.status.idle":"2021-08-11T14:02:13.519470Z","shell.execute_reply.started":"2021-08-11T14:02:09.892925Z","shell.execute_reply":"2021-08-11T14:02:13.518321Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"model.save_pretrained('./')","metadata":{"execution":{"iopub.status.busy":"2021-08-11T14:02:13.520919Z","iopub.execute_input":"2021-08-11T14:02:13.521363Z","iopub.status.idle":"2021-08-11T14:02:33.124590Z","shell.execute_reply.started":"2021-08-11T14:02:13.521317Z","shell.execute_reply":"2021-08-11T14:02:33.123592Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"# model.load_state_dict(torch.load(\"save.pt\"))  #model.load_state_dict()函数把加载的权重复制到模型的权重中去","metadata":{"execution":{"iopub.status.busy":"2021-08-11T14:02:33.126005Z","iopub.execute_input":"2021-08-11T14:02:33.126469Z","iopub.status.idle":"2021-08-11T14:02:33.133988Z","shell.execute_reply.started":"2021-08-11T14:02:33.126423Z","shell.execute_reply":"2021-08-11T14:02:33.130830Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"# Predict","metadata":{}},{"cell_type":"code","source":"# Load the dataset into a pandas dataframe.\ndf_predict = pd.read_csv(\"../input/sentiment-classification-on-movie-reviews/test.csv\")\n\n# Report the number of sentences.\nprint('Number of test sentences: {:,}\\n'.format(df_predict.shape[0]))\n\nprint(df_predict.head())\nprint(df_predict.tail())\n\n# Create sentence lists\nsentences_predict = df_predict.review.values","metadata":{"execution":{"iopub.status.busy":"2021-08-11T14:02:33.135654Z","iopub.execute_input":"2021-08-11T14:02:33.136095Z","iopub.status.idle":"2021-08-11T14:02:38.014353Z","shell.execute_reply.started":"2021-08-11T14:02:33.136049Z","shell.execute_reply":"2021-08-11T14:02:38.012531Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"Number of test sentences: 29,341\n\n      ID                                             review\n0  22622  Robert Lansing plays a scientist experimenting...\n1  10162  Well I've enjoy this movie, even though someti...\n2  17468  First things first - though I believe Joel Sch...\n3  42579  I watched this movie on the grounds that Amber...\n4    701  A certain sexiness underlines even the dullest...\n          ID                                             review\n29336  30370  It is difficult to rate a writer/director's fi...\n29337  18654  After watching this movie once, it quickly bec...\n29338  47985  Even though i sat and watched the whole thing,...\n29339   9866  Warning Spoilers following. Superb recreation ...\n29340  35559  My, my, my: Peter Cushing and Donald Pleasance...\n","output_type":"stream"}]},{"cell_type":"code","source":"# 去除特定字元符號\nfor i in range(0,29341):\n    sentences_predict[i] = re.sub('<br />', ' ', sentences_predict[i])\n    sentences_predict[i] = re.sub('--(-+)', '--', sentences_predict[i])\n    #sentences_predict[i] = re.sub('\\((2([0-9][0-9][0-9])|1([0-9][0-9][0-9]))\\)', ' ', sentences_predict[i])\n    sentences_predict[i] = re.sub('\\?\\?(\\?+)', '??', sentences_predict[i])\n    sentences_predict[i] = re.sub('!!(!+)', '!!', sentences_predict[i])\n    \n    sentences_predict[i] = re.sub('I\\'m', 'I am', sentences_predict[i])\n    sentences_predict[i] = re.sub('can\\'t', 'can not', sentences_predict[i])\n    sentences_predict[i] = re.sub('n\\'t', ' not', sentences_predict[i])\n    if( i % 10000 == 0 ):\n        print(i)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T14:23:57.824085Z","iopub.execute_input":"2021-08-11T14:23:57.824488Z","iopub.status.idle":"2021-08-11T14:23:58.338440Z","shell.execute_reply.started":"2021-08-11T14:23:57.824456Z","shell.execute_reply":"2021-08-11T14:23:58.337273Z"},"trusted":true},"execution_count":62,"outputs":[{"name":"stdout","text":"0\n10000\n20000\n","output_type":"stream"}]},{"cell_type":"code","source":"# for i in range(0,29341): # 句子長度超過270字的，取最前段的270字    \n#     if(len(sentences_predict[i].split())>270):\n#         sentences_temp = sentences_predict[i].split()\n#         sentences_predict[i] = \" \".join(sentences_temp[0:270])","metadata":{"execution":{"iopub.status.busy":"2021-08-11T14:02:40.635714Z","iopub.execute_input":"2021-08-11T14:02:40.636148Z","iopub.status.idle":"2021-08-11T14:02:42.948328Z","shell.execute_reply.started":"2021-08-11T14:02:40.636083Z","shell.execute_reply":"2021-08-11T14:02:42.946867Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"# Tokenize all of the sentences and map the tokens to thier word IDs.\ninput_ids_predict = []\nidx_counter = 0\n# For every sentence...\nfor sent in sentences_predict:\n    # `encode` will:\n    #   (1) Tokenize the sentence.\n    #   (2) Prepend the `[CLS]` token to the start.\n    #   (3) Append the `[SEP]` token to the end.\n    #   (4) Map tokens to their IDs.\n    encoded_sent = tokenizer.encode(\n                        sent,                      # Sentence to encode.\n                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n                   )\n    \n    input_ids_predict.append(encoded_sent)\n    if(idx_counter%5000 == 0):\n        print(idx_counter)\n    idx_counter += 1","metadata":{"execution":{"iopub.status.busy":"2021-08-11T14:24:03.363074Z","iopub.execute_input":"2021-08-11T14:24:03.363466Z","iopub.status.idle":"2021-08-11T14:24:40.638811Z","shell.execute_reply.started":"2021-08-11T14:24:03.363435Z","shell.execute_reply":"2021-08-11T14:24:40.637734Z"},"trusted":true},"execution_count":63,"outputs":[{"name":"stdout","text":"0\n5000\n10000\n15000\n20000\n25000\n","output_type":"stream"}]},{"cell_type":"code","source":"# Tokenized Sentences長度超過350字的，取最前段的350字\nfor i in range(0,29341):\n    if(len(input_ids_predict[i])>350):\n        temp = input_ids_predict[i][0:349]\n        temp.append(2)\n        input_ids_predict[i] = temp","metadata":{"execution":{"iopub.status.busy":"2021-08-11T14:24:47.282939Z","iopub.execute_input":"2021-08-11T14:24:47.283317Z","iopub.status.idle":"2021-08-11T14:24:47.334121Z","shell.execute_reply.started":"2021-08-11T14:24:47.283284Z","shell.execute_reply":"2021-08-11T14:24:47.333120Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"max_seq_len_predict = max([len(sen) for sen in input_ids_predict])\nprint('Max sentence length: ', max_seq_len_predict)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T14:24:49.419556Z","iopub.execute_input":"2021-08-11T14:24:49.419947Z","iopub.status.idle":"2021-08-11T14:24:49.433465Z","shell.execute_reply.started":"2021-08-11T14:24:49.419912Z","shell.execute_reply":"2021-08-11T14:24:49.429191Z"},"trusted":true},"execution_count":65,"outputs":[{"name":"stdout","text":"Max sentence length:  350\n","output_type":"stream"}]},{"cell_type":"code","source":"# get length of all the encode\nseq_len_predict_encode = [len(i) for i in input_ids_predict]\n\npd.Series(seq_len_predict_encode).hist(bins = 30)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T14:24:51.659539Z","iopub.execute_input":"2021-08-11T14:24:51.659906Z","iopub.status.idle":"2021-08-11T14:24:51.970382Z","shell.execute_reply.started":"2021-08-11T14:24:51.659876Z","shell.execute_reply":"2021-08-11T14:24:51.969134Z"},"trusted":true},"execution_count":66,"outputs":[{"execution_count":66,"output_type":"execute_result","data":{"text/plain":"<AxesSubplot:>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"# for i in range(0,29341):\n#     if(len(input_ids_predict[i])==500):\n#         print(i,': encoded len:',len(input_ids_predict[i]),'sentence len:',len(sentences_predict[i].split()))\n#         print(sentences_predict[i])","metadata":{"execution":{"iopub.status.busy":"2021-08-11T14:03:19.329549Z","iopub.execute_input":"2021-08-11T14:03:19.329950Z","iopub.status.idle":"2021-08-11T14:03:19.334619Z","shell.execute_reply.started":"2021-08-11T14:03:19.329907Z","shell.execute_reply":"2021-08-11T14:03:19.333467Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"MAX_LEN_predict = max_seq_len_predict\n\nprint('\\nPadding/truncating all sentences to %d values...' % MAX_LEN_predict)\nprint('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n\n# Pad our input tokens\ninput_ids_predict = pad_sequences(input_ids_predict, maxlen=MAX_LEN_predict, \n                          dtype=\"long\", truncating=\"post\", padding=\"post\")\nprint('Padding done.')\n\n# Create attention masks\nattention_masks_predict = []\n\n# Create a mask of 1s for each token followed by 0s for padding\nfor seq in input_ids_predict:\n    seq_mask = [float(i>0) for i in seq]\n    attention_masks_predict.append(seq_mask)\n\nprint('Create attention masks done.')","metadata":{"execution":{"iopub.status.busy":"2021-08-11T14:25:11.166205Z","iopub.execute_input":"2021-08-11T14:25:11.166574Z","iopub.status.idle":"2021-08-11T14:25:24.728258Z","shell.execute_reply.started":"2021-08-11T14:25:11.166542Z","shell.execute_reply":"2021-08-11T14:25:24.727006Z"},"trusted":true},"execution_count":67,"outputs":[{"name":"stdout","text":"\nPadding/truncating all sentences to 350 values...\n\nPadding token: \"<pad>\", ID: 1\nPadding done.\nCreate attention masks done.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Convert to tensors.\nprediction_inputs = torch.tensor(input_ids_predict).to(device)\nprediction_masks = torch.tensor(attention_masks_predict).to(device)\n# prediction_labels = torch.tensor(labels_predict)\n\n# Set the batch size.  \nbatch_size = 8 \n\n# Create the DataLoader.\nprediction_data = TensorDataset(prediction_inputs, prediction_masks)#, prediction_labels)\nprediction_sampler = SequentialSampler(prediction_data)\nprediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T14:25:28.188283Z","iopub.execute_input":"2021-08-11T14:25:28.188640Z","iopub.status.idle":"2021-08-11T14:25:29.203050Z","shell.execute_reply.started":"2021-08-11T14:25:28.188609Z","shell.execute_reply":"2021-08-11T14:25:29.202030Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"# Prediction on test set\n\nprint('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n\n# Put model in evaluation mode\nmodel.eval()\n\n# Tracking variables \npred , true_labels = [], []\n\ncounter_predict = 0\n        \n# Predict \nfor batch in prediction_dataloader:\n    \n    if counter_predict % 200 == 0:\n        print('{:>5,}  of  {:>5,}. '.format(counter_predict, len(prediction_dataloader)) )\n    \n    # Add batch to GPU\n    batch = tuple(t.to(device) for t in batch)\n\n    # Unpack the inputs from our dataloader\n    #b_input_ids, b_input_mask, b_labels = batch\n    b_input_ids, b_input_mask = batch\n\n    # Telling the model not to compute or store gradients, saving memory and \n    # speeding up prediction\n    with torch.no_grad():\n        # Forward pass, calculate logit predictions\n        outputs = model(b_input_ids, token_type_ids=None, \n                          attention_mask=b_input_mask)\n\n    logits = outputs[0]\n\n    # Move logits and labels to CPU\n    logits = logits.detach().cpu().numpy()\n#     label_ids = b_labels.to('cpu').numpy()\n\n    # Store predictions and true labels\n    pred.append(logits)\n    #true_labels.append(label_ids)\n    counter_predict += 1\n\nprint('DONE.')","metadata":{"execution":{"iopub.status.busy":"2021-08-11T14:25:33.148514Z","iopub.execute_input":"2021-08-11T14:25:33.148902Z","iopub.status.idle":"2021-08-11T14:45:12.773006Z","shell.execute_reply.started":"2021-08-11T14:25:33.148869Z","shell.execute_reply":"2021-08-11T14:45:12.771917Z"},"trusted":true},"execution_count":69,"outputs":[{"name":"stdout","text":"Predicting labels for 29,341 test sentences...\n    0  of  3,668. \n  200  of  3,668. \n  400  of  3,668. \n  600  of  3,668. \n  800  of  3,668. \n1,000  of  3,668. \n1,200  of  3,668. \n1,400  of  3,668. \n1,600  of  3,668. \n1,800  of  3,668. \n2,000  of  3,668. \n2,200  of  3,668. \n2,400  of  3,668. \n2,600  of  3,668. \n2,800  of  3,668. \n3,000  of  3,668. \n3,200  of  3,668. \n3,400  of  3,668. \n3,600  of  3,668. \nDONE.\n","output_type":"stream"}]},{"cell_type":"code","source":"# import numpy as np\n# pred = [[1,4],[2,8]]","metadata":{"execution":{"iopub.status.busy":"2021-08-11T14:23:13.422920Z","iopub.execute_input":"2021-08-11T14:23:13.423352Z","iopub.status.idle":"2021-08-11T14:23:13.430694Z","shell.execute_reply.started":"2021-08-11T14:23:13.423309Z","shell.execute_reply":"2021-08-11T14:23:13.429645Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"# preds = np.concatenate(pred)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T14:23:13.432483Z","iopub.execute_input":"2021-08-11T14:23:13.432945Z","iopub.status.idle":"2021-08-11T14:23:13.440831Z","shell.execute_reply.started":"2021-08-11T14:23:13.432897Z","shell.execute_reply":"2021-08-11T14:23:13.439837Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"# print(preds)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T14:23:13.442449Z","iopub.execute_input":"2021-08-11T14:23:13.442921Z","iopub.status.idle":"2021-08-11T14:23:13.451554Z","shell.execute_reply.started":"2021-08-11T14:23:13.442878Z","shell.execute_reply":"2021-08-11T14:23:13.450469Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"# labels = np.argmax(preds, axis =1)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T14:23:13.453489Z","iopub.execute_input":"2021-08-11T14:23:13.454049Z","iopub.status.idle":"2021-08-11T14:23:13.461787Z","shell.execute_reply.started":"2021-08-11T14:23:13.453992Z","shell.execute_reply":"2021-08-11T14:23:13.460676Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"# deal with predict result\npred_flat = []\nfor i in range( 0, len(prediction_dataloader) ):\n    pred_flat.append( np.argmax(pred[i], axis=1).flatten() ) # 沿著行(axis=0)或列(axis=1)查詢最大值的索引號\n\npred_list = []\nfor i in pred_flat:\n    for j in i:\n        pred_list.append(j)\n# Convert List to numpy Arrays\npredictions = np.array(pred_list)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T14:45:29.571315Z","iopub.execute_input":"2021-08-11T14:45:29.571677Z","iopub.status.idle":"2021-08-11T14:45:29.623377Z","shell.execute_reply.started":"2021-08-11T14:45:29.571646Z","shell.execute_reply":"2021-08-11T14:45:29.622351Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"print(pred_flat[0:2])\nprint(predictions[0:32])\nprint(len(predictions))","metadata":{"execution":{"iopub.status.busy":"2021-08-11T14:45:32.395056Z","iopub.execute_input":"2021-08-11T14:45:32.395463Z","iopub.status.idle":"2021-08-11T14:45:32.404484Z","shell.execute_reply.started":"2021-08-11T14:45:32.395431Z","shell.execute_reply":"2021-08-11T14:45:32.403037Z"},"trusted":true},"execution_count":71,"outputs":[{"name":"stdout","text":"[array([0, 1, 0, 0, 1, 0, 1, 1]), array([0, 0, 1, 0, 0, 0, 1, 1])]\n[0 1 0 0 1 0 1 1 0 0 1 0 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 0 1 0]\n29341\n","output_type":"stream"}]},{"cell_type":"code","source":"# write predict result to file\nID_predict = df_predict.ID.values\n\nresults = pd.DataFrame({\"ID\":ID_predict,\n                        \"sentiment\":predictions})\n\nresults.to_csv(f'/kaggle/working/results.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T14:45:34.663248Z","iopub.execute_input":"2021-08-11T14:45:34.663645Z","iopub.status.idle":"2021-08-11T14:45:34.726636Z","shell.execute_reply.started":"2021-08-11T14:45:34.663611Z","shell.execute_reply":"2021-08-11T14:45:34.725652Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"results.describe()","metadata":{"execution":{"iopub.status.busy":"2021-08-11T14:45:38.596623Z","iopub.execute_input":"2021-08-11T14:45:38.596994Z","iopub.status.idle":"2021-08-11T14:45:38.622999Z","shell.execute_reply.started":"2021-08-11T14:45:38.596962Z","shell.execute_reply":"2021-08-11T14:45:38.621787Z"},"trusted":true},"execution_count":73,"outputs":[{"execution_count":73,"output_type":"execute_result","data":{"text/plain":"                 ID     sentiment\ncount  29341.000000  29341.000000\nmean   29332.588903      0.516342\nstd    16878.341879      0.499741\nmin        0.000000      0.000000\n25%    14759.000000      0.000000\n50%    29332.000000      1.000000\n75%    43859.000000      1.000000\nmax    58679.000000      1.000000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>29341.000000</td>\n      <td>29341.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>29332.588903</td>\n      <td>0.516342</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>16878.341879</td>\n      <td>0.499741</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>14759.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>29332.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>43859.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>58679.000000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# check class distribution\nresults.sentiment.value_counts(normalize = True)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T14:45:45.681919Z","iopub.execute_input":"2021-08-11T14:45:45.682313Z","iopub.status.idle":"2021-08-11T14:45:45.692746Z","shell.execute_reply.started":"2021-08-11T14:45:45.682278Z","shell.execute_reply":"2021-08-11T14:45:45.691564Z"},"trusted":true},"execution_count":74,"outputs":[{"execution_count":74,"output_type":"execute_result","data":{"text/plain":"1    0.516342\n0    0.483658\nName: sentiment, dtype: float64"},"metadata":{}}]}]}